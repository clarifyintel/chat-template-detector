model: meta-llama/Llama-2-7b-chat-hf
temperature: 0.7
max_tokens: 512
chat_template: "[INST] {prompt} [/INST]"
